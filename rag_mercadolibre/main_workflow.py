import streamlit as st
import re 
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# Import Servientrega checker function
from servientrega_checker import check_servientrega_status 

# --- Configuraci√≥n de la app
st.set_page_config(page_title="üõçÔ∏è Asistente de Cat√°logo Meli", layout="centered")
st.title("üõçÔ∏è Asistente del Cat√°logo MercadoLibre")

# ---  Embeddings
@st.cache_resource
def get_embeddings():
    
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ---  Carga del vector store (aseg√∫rate de tener ./chroma_db generado)
@st.cache_resource
def get_vectorstore():
    embeddings = get_embeddings()
    return Chroma(persist_directory="./chroma_db", embedding_function=embeddings)

# ---  Cargar el modelo local
@st.cache_resource
def get_llm():
    return OllamaLLM(model="mistral", temperature=0.3)

# ---  Construir el RAG pipeline
@st.cache_resource
def build_rag_chain():
    vectorstore = get_vectorstore()
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    llm = get_llm()

    prompt = ChatPromptTemplate.from_template("""
    Eres un asistente experto en el cat√°logo de productos.
    Responde solo con base en la informaci√≥n del contexto.
    Si no sabes la respuesta, realiza un resumen de los productos en el cat√°logo.
    Lo mismo aplica si la consulta es vaga o general. Si hablan de productos, tambi√©n se refieren a figuras"

    Contexto:
    {context}

    Pregunta:
    {question}

    Respuesta en espa√±ol:
    """)

    return (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
    )

rag_chain = build_rag_chain()

# ---  Interfaz de usuario y L√≥gica de Ramificaci√≥n
query = st.text_input("Haz tu pregunta sobre los productos o el estado de tu env√≠o (ej: rastrea gu√≠a 2259180939):")

if query:
    # L√≥gica de Ramificaci√≥n para el Rastreo de Servientrega
    
    # 1. Convertir la consulta a min√∫sculas para un manejo m√°s f√°cil
    lower_query = query.lower()
    
    # 2. Buscar un patr√≥n de n√∫mero de gu√≠a (10 d√≠gitos)
    # Patr√≥n: \d{10} busca exactamente 10 d√≠gitos.
    tracking_number_match = re.search(r'\d{10}', lower_query)
    
    # --- RAMIFICACI√ìN DE EJECUCI√ìN ---
    if tracking_number_match:
        #  Caso A: Un n√∫mero de rastreo de 10 d√≠gitos fue encontrado. 
        # Ejecutar el checker (PRIORIDAD AL RASTREO).
        tracking_number = tracking_number_match.group(0)
        
        st.info(f"Detectada consulta de rastreo. Buscando estado de gu√≠a: **{tracking_number}**")
        
        with st.spinner(f"Contactando a Servientrega para la gu√≠a {tracking_number}..."):
            # Llama a tu funci√≥n del otro archivo
            status_result = check_servientrega_status(tracking_number)
        
        st.write("### üöö Estado del Env√≠o:")
        
        # Muestra el resultado
        if "ERROR" in status_result:
            st.error(status_result)
        else:
            # Output limpio y exitoso basado en tu prueba
            st.success(f"Gu√≠a **{tracking_number}** - **{status_result}**")
            
    else:
        # üìö Caso B: No se encontr√≥ un n√∫mero de 10 d√≠gitos. Ejecutar el pipeline RAG.
        st.info("Detectada consulta de cat√°logo. Buscando con RAG...")
        
        with st.spinner("Buscando en el cat√°logo..."):
            response = rag_chain.invoke(query)
            
        st.write("### üí¨ Respuesta del Cat√°logo:")
        st.success(response)