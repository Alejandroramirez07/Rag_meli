import streamlit as st
import re
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda # <-- Se agreg칩 RunnableLambda
from langchain_core.messages import HumanMessage, AIMessage
from servientrega_checker import check_servientrega_status 

# --- Configuraci칩n de Streamlit UI ---
st.set_page_config(page_title="游뱄 Asistente de Cat치logo Meli", layout="wide")
st.title("游 Asistente de Cat치logo Meli")
st.caption("Preg칰ntame sobre productos o rastrea un env칤o (ej: rastrea gu칤a 2259180939)")

# --- Configuraci칩n de LangChain/RAG (Aseg칰rate de que Ollama est칠 corriendo Mistral) ---
@st.cache_resource
def setup_rag_chain():
    # 1. Embeddings
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # 2. Vector DB (Cargando la DB persistente)
    vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
    
    # Recuperaci칩n y contexto
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4}) 

    # 3. LLM local
    llm = OllamaLLM(model="mistral", temperature=0.3, num_ctx=1024)

    # 4. Prompt: Usamos from_template para manejar el historial como texto plano.
    prompt_template = """
    Eres un asistente experto en el cat치logo de productos de la tienda.
    Tu objetivo es responder a las preguntas del usuario sobre el cat치logo.

    Instrucciones de respuesta:
    1. Responde concisa y solo con base en la informaci칩n del contexto.
    2. Si el contexto no es suficiente para responder la pregunta actual, haz un resumen de los productos mencionados en el contexto.

    ---
    Historial de Conversaci칩n:
    {history}
    ---
    
    Contexto de Cat치logo (Documentos RAG):
    {context}
    
    Pregunta Actual:
    {question}

    Respuesta en espa침ol:
    """
    prompt = ChatPromptTemplate.from_template(prompt_template)
    
    # Serializa la lista de documentos en una sola cadena de texto.
    def format_docs(docs):
        # Los une usando un separador claro
        return "\n\n---\n\n".join(doc.page_content for doc in docs)


    # 5. RAG Chain:
    # RunnableLambda para extraer expl칤citamente solo el valor de la clave.
    rag_chain = (
        {
            # 'question' (string) al retriever
            "context": RunnableLambda(lambda x: x["question"]) | retriever | format_docs,
            
            # Pasa el valor de 'question' y 'history' directamente al prompt
            "question": RunnableLambda(lambda x: x["question"]),   
            "history": RunnableLambda(lambda x: x["history"])     
        }
        | prompt
        | llm
    )
    return rag_chain

# Inicializar la chain global
try:
    rag_chain = setup_rag_chain()
except Exception as e:
    st.error(f"Error al configurar la cadena RAG. 쮼st치 Ollama corriendo y el modelo 'mistral' cargado? Error: {e}")
    st.stop()


# --- 1. Inicializaci칩n del Historial de Chat ---
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "춰Hola! Soy tu asistente de Cat치logo Meli. Preg칰ntame sobre nuestros productos o dame un n칰mero de gu칤a para rastrear tu env칤o."}
    ]

# --- 2. Renderizar Mensajes Anteriores ---
for msg in st.session_state.messages:
    # Usamos st.chat_message para mostrar los iconos de 'user' o 'assistant'
    st.chat_message(msg["role"]).write(msg["content"])


# --- 3. Manejo de Nueva Entrada del Usuario ---
if prompt := st.chat_input("Escribe tu pregunta o n칰mero de gu칤a aqu칤..."):
    
    # A침adir el mensaje del usuario al historial y mostrarlo inmediatamente
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # Iniciar el contenedor para la respuesta del asistente
    with st.chat_message("assistant"):
        
        # Preparamos el contexto para el LLM: El historial y la pregunta
        history_text = ""
        for msg in st.session_state.messages:
            # Excluimos el mensaje actual para que vaya en la variable {question}
            if msg["content"] != prompt:
                 history_text += f"[{msg['role'].capitalize()}]: {msg['content']}\n"
        
        # Si el historial est치 vac칤o (primer mensaje), evitamos enviar una l칤nea de m치s
        if not history_text:
             history_text = "N/A"
        
        # --- L칍GICA DE BIFURCACI칍N H칈BRIDA (RAG o Herramienta) ---
        
        # 1. Intentar detectar un n칰mero de gu칤a de Servientrega (10 d칤gitos)
        tracking_number_match = re.search(r'\b(\d{10})\b', prompt)
        
        if tracking_number_match:
            tracking_number = tracking_number_match.group(1)
            
            # --- Ruta de Herramienta (Selenium) ---
            response_placeholder = st.empty()
            response_placeholder.info(f"Ruta de Herramienta: Detectado n칰mero de gu칤a **{tracking_number}**.")
            
            with st.spinner(f"游깷 Consultando estado de env칤o para {tracking_number} en Servientrega..."):
                try:
                    status_result = check_servientrega_status(tracking_number)
                    final_response = f"**游닍 Estado del Env칤o {tracking_number}:**\n\n{status_result}"
                    response_placeholder.success("Consulta completada. Mostrando resultado.")

                except Exception as e:
                    final_response = f"丘멆잺 Hubo un error al intentar rastrear el env칤o {tracking_number}. Por favor, verifica el n칰mero e intenta m치s tarde. Detalle del error: {e}"
                    response_placeholder.error("Error en la consulta de rastreo.")
            
            st.markdown(final_response)

        else:
            # --- Ruta RAG (Consulta de Cat치logo) ---
            response_placeholder = st.empty()
            response_placeholder.info("Ruta RAG: Consultando Cat치logo de Productos...")
            
            try:
                # La nueva estructura RAG requiere ambas claves: question y history
                chain_input = {
                    "question": prompt, 
                    "history": history_text
                }
                
                with st.spinner("游 Generando respuesta con Ollama Mistral..."):
                    final_response = rag_chain.invoke(chain_input)
                
                response_placeholder.success("Respuesta generada.")

            except Exception as e:
                # El error detallado para el desarrollador
                print(f"ERROR RAG: {e}")
                final_response = f"丘멆잺 Ocurri칩 un error al comunicarse con el modelo LLM. Aseg칰rate de que Ollama est칠 activo. Error: '{e}'"
                response_placeholder.error("Error en la generaci칩n RAG.")
                
            st.markdown(final_response)


        # --- 4. Almacenar la Respuesta del Asistente en el Historial ---
        st.session_state.messages.append({"role": "assistant", "content": final_response})