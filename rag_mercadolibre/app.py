import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# --- Configuraci√≥n de la app
st.set_page_config(page_title="üõçÔ∏è Asistente de Cat√°logo Meli", layout="centered")
st.title("üõçÔ∏è Asistente del Cat√°logo MercadoLibre")

# --- 1Ô∏è‚É£ Embeddings
@st.cache_resource
def get_embeddings():
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# --- 2Ô∏è‚É£ Carga del vector store (aseg√∫rate de tener ./chroma_db generado)
@st.cache_resource
def get_vectorstore():
    embeddings = get_embeddings()
    return Chroma(persist_directory="./chroma_db", embedding_function=embeddings)

# --- 3Ô∏è‚É£ Cargar el modelo local
@st.cache_resource
def get_llm():
    return OllamaLLM(model="mistral", temperature=0.1)

# --- 4Ô∏è‚É£ Construir el RAG pipeline
@st.cache_resource
def build_rag_chain():
    vectorstore = get_vectorstore()
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
    llm = get_llm()

    prompt = ChatPromptTemplate.from_template("""
    Eres un asistente experto en el cat√°logo de productos.
    Responde solo con base en la informaci√≥n del contexto.
    Si no sabes la respuesta, di: "No tengo esa informaci√≥n en el cat√°logo."

    Contexto:
    {context}

    Pregunta:
    {question}

    Respuesta en espa√±ol:
    """)

    return (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
    )

rag_chain = build_rag_chain()

# --- 5Ô∏è‚É£ Interfaz de usuario
query = st.text_input("Haz tu pregunta sobre los productos:")

if query:
    with st.spinner("Buscando en el cat√°logo..."):
        response = rag_chain.invoke(query)
    st.write("### üí¨ Respuesta:")
    st.success(response)
